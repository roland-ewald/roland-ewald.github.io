---
title:  "New Anthropic papers on 'mind-reading' LLMs"
date:   2025-04-12
tags: ai computer-science explainability biology neuroscience llm medicine ai-alignment
---

Some days ago [Anthropic](https://en.wikipedia.org/wiki/Anthropic) released two new papers on improving the explainability of LLMs;[^ai-2027] [here is their summary](https://www.anthropic.com/research/tracing-thoughts-language-model). One paper presents a new method called ['Circuit Tracing'](https://transformer-circuits.pub/2025/attribution-graphs/methods.html), the other paper presents the ['biological' insights](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) gained from analyzing their Claude 3.5 model with this new method.[^hype]

Developing such tools is important, because analyzing the execution of an LLM query is still cumbersome (from [here](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=It%20currently%20takes%20a%20few%20hours%20of%20human%20effort%20to%20understand%20the%20circuits%20we%20see%2C%20even%20on%20prompts%20with%20only%20tens%20of%20words)):

> It currently takes a few hours of human effort to understand the circuits we see, even on prompts with only tens of words.

If we want to ensure [AI alignment](https://en.wikipedia.org/wiki/AI_alignment), we need our tooling for the analysis of LLM 'thinking' processes to be as effective and efficient as possible (so that they are actually used in practice).

## Method: brief overview

1. At first, so called **Cross-Layer-Transcoders** are trained on each layer to approximate the LLM's output at this layer (and can feed this info to all following transcoders across layers, hence the name). Crucially, these have _more_ structure than the multi-layer perceptrons in the LLM and each neuron represents a _feature_.[^feature-number]

2. These cross-level transcoders can now replace the original LLM, which yields a **Replacement Model** that can be fairly precise: if trained with suitable settings, it can predict the next word that would be generated by the LLM with 50% probability.[^probability]

3. For a given query, a **Local Replacement Model** can now be constructed, by comparing the results with the execution by the LLM (to reduce errors) and by also incorporating its [attention](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) when generating the response.

4. This yields an **Attribution Graph** of feature notes that can now be pruned and improved (e.g. by defining 'super nodes'), but it principle allows us to see what the LLM 'thought' while generating the token stream.

All this is quite elaborate, but the bottom line is: with some additional work we can create a model that is easier to interpret (a 'replacement model'), and by comparing a calculation by the full model with the same calculation in the replacement model we can eventually map the execution of a query onto a detailed (but not _too_ large) graph where each node represents a specific 'feature' (i.e., concept). This so-called attribution graph can now be analyzed further, and the Anthropic team shared some insights from these analyses in the [companion paper](https://transformer-circuits.pub/2025/attribution-graphs/biology.html).

## Some interesting findings

### Claude _plans_ when constructing rhymes

It was unclear whether LLMs even _can_ 'think ahead', and to what extent they actually do. LLMs predict the next word on the basis of _previous_ words, so you could argue they are inherently 'backwards-looking' when generating an answer. But the Anthropic team illustrates in this figure how you can see Claude planning ahead when constructing a rhyme:

![](https://www-cdn.anthropic.com/images/4zrzovbb/website/7032ed7db85b8cd3efe70a89deaf4f15bfe8fc05-1650x900.png?w=2048&q=100){: width="768"}

### Claude _abstracts_ from input languages

If you have ever wondered how human brains encode the knowledge of multiple languages, you can now _see_ how Claude does this, i.e. how it answers the same question asked in different languages. It starts by mapping the questions to the 'concept space', and then translates the result back to the language being used:

![](https://www-cdn.anthropic.com/images/4zrzovbb/website/e0e156ea6c912a385d66ed562187fced8c392a58-1650x750.png?w=2048&q=100){: width="768"}

So, given a sufficient text corpus to become 'fluent' in one language, this would mean the performance of Claude should be _comparable_ across languages, even in fields where there may not be too much existing content for training. Practically speaking, this means I should be able to get the same answer quality when I'm asking a research question in German instead of English, even though the training set is heavily biased towards English (e.g. for biomedical literature).
There is also a scaling effect involved: apparently, larger models share _more_ concepts across languages.[^universal-grammar]

### Claude can be caught cheating

The authors show [some nice examples of 'motivated reasoning'](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot), including an [interactive visualization of the attribution graph](https://transformer-circuits.pub/2025/attribution-graphs/biology.html?slug=cot-unfaithful-math-4#cot-circuits-svg), where Claude just _wants_ to agree with you on the answer to a mathematical question, and thus lies to you. 
They even can distinguish this from benign hallucinations (i.e., 'bullshitting'), and this can be quite useful for safety testing and audits (from [here](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=The%20ability%20to%20trace%20Claude%27s%20actual%20internal%20reasoning%E2%80%94and%20not%20just%20what%20it%20claims%20to%20be%20doing%E2%80%94opens%20up%20new%20possibilities%20for%20auditing%20AI%20systems.)):

> The ability to trace Claude's actual internal reasoning—and not just what it claims to be doing—opens up new possibilities for auditing AI systems. 

### Claude's behavior during a jailbreak can be explained

The authors also [show how](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-jailbreak) this technique can be used to analyze jailbreaking attempts, i.e. making the LLM answer a question it should not answer at all (from [here](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=The%20model%20only%20managed%20to%20pivot%20to%20refusal%20after%20completing%20a%20grammatically%20coherent%20sentence%20(and%20thus%20having%20satisfied%20the%20pressure%20from%20the%20features%20that%20push%20it%20towards%20coherence).)):

> The model only managed to pivot to refusal after completing a grammatically coherent sentence (and thus having satisfied the pressure from the features that push it towards coherence).

These insights into the 'psychology of an LLM' could be used to harden future LLMs against such attacks.

### Claude is _reasoning_ at least in some limited sense

By looking at the concepts in the attribution graph, it becomes clear that Claude is _not_ just memorizing facts, but instead considers several abstract concepts to generate an answer. Here is a simple example from the study:

![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffd2e125879ab993949017e03e3465a12fda884bf-1650x857.png&w=3840&q=75){: width="768"}

From this, [the authors conclude](https://www.anthropic.com/research/tracing-thoughts-language-model#:~:text=In%20other%20words%2C%20the%20model%20is%20combining%20independent%20facts%20to%20reach%20its%20answer%20rather%20than%20regurgitating%20a%20memorized%20response.):

> In other words, the model is combining independent facts to reach its answer rather than regurgitating a memorized response.

To frame this in terms of well-established computer science concepts, even if an LLM may not be _thinking_ (that's a rather philosophical question :-), creating an LLM may just be a great way to extract huge, fuzzy ontologies---as well as all relevant 'reasoning algorithms' to work with them---from a large text corpus.

Also considering the [examples for medical diagnoses](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-medical), I wonder if attribution graphs could also be used to fine-tune and 'fact-check' existing ontologies and catalogues in medicine, such as the [Human Phenotype Ontology (HPO)](https://hpo.jax.org/) or the [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html). 
Such an approach might even find the 'unknown unknowns', i.e. gaps in the ontology: by applying attribution graphs to a larger set of medical diagnoses, I would expect that the learned concepts e.g. regarding a patient's symptoms should start to closely match the structures in manually curated ontologies.

## Summary

I find the insights by the Anthropic team fascinating because _one_ interpretation of the results could be that LLMs may, at least to some extent, exhibiting patterns that approximate the way _natural_ neural networks process information (for example human brains may deal with speaking different languages in a similar way).

It also makes me slightly more hopeful regarding 'AI safety', but this is still a [wicked problem](https://en.wikipedia.org/wiki/Wicked_problem). We can't easily _ensure_ that this kind of analysis is done, and done _correctly_, by well-meaning actors.[^thought-police]

--

[^hype]: If you still think LLMs are marketing hype, consider what Tyler Cowen[^tyler-cowen] [has to say](https://marginalrevolution.com/marginalrevolution/2025/02/deep-research.html) about OpenAI's [o3 model](https://en.wikipedia.org/wiki/OpenAI_o3): _"I think of the quality as comparable to having a good PhD-level research assistant, and sending that person away with a task for a week or two, or maybe more. Except Deep Research does the work in five or six minutes.  And it does not seem to make errors, due to the quality of the embedded o3 model."_.

[^tyler-cowen]: [Tyler Cowen](https://en.wikipedia.org/wiki/Tyler_Cowen) is an economics professor at [George Mason university](https://economics.gmu.edu/people/tcowen) and one of the two [marginalrevolution](https://marginalrevolution.com/) blog authors; see his [recent profile in the '1843' magazine](https://www.economist.com/1843/2025/02/28/tyler-cowen-the-man-who-wants-to-know-everything) (paywalled) for more background.

[^probability]: This may seem bad, but consider that there are _many_ words, so predicting the next word with 50% probability is far from random chance.

[^ai-2027]: I was reading through the papers and preparing this post when the [AI-2027 scenario was released](/2025/04/05/ai-2027.html). Considering well-researched scenarios like this makes this work by the Anthropic team on AI alignment all the more important.

[^universal-grammar]: With enough scale, will this eventually converge towards a [universal grammar](https://en.wikipedia.org/wiki/Universal_grammar)? I hope some computational linguists are already looking at this... :-)

[^feature-number]: There are up to 30 million such neurons when applied to a state-of-the-art model like Claude 3.5.

[^thought-police]: You could also frame this whole approach as 'thought police for AI agents' and use it, for example, to make sure your new evil LLM keeps its true agenda hidden from the AI safety auditor checking [the 'three Hs' (harmless, honest, helpful)](https://www.neilsahota.com/harmless-honest-and-helpful-ai-aligning-ai-the-right-way/).