---
title:  "AI-2027: a month-by-month prediction for AI development"
date:   2025-04-05
tags: ai prediction safety security podcast ai-alignment future
---

Some well-known AI experts and forecasters[^scott-alexander] prepared a **detailed month-by-month prediction** on how the road to 'Artificial General Intelligence' might look like: **[AI-2027](https://ai-2027.com/)** ([PDF](https://ai-2027.com/scenario.pdf)).[^ai-futures]

You could consider this a quantitative update for last year's [outlook on 'the next decade in AI' (by Leopold Aschbrenner)](https://situational-awareness.ai/), and just as the [recent whitepaper on AI geopolitics](/2025/03/08/links.html#how-to-deal-with-superintelligence) it is mostly a sobering read,[^gwern] with little reason for optimism, even though the authors _try_ to present an 'alternative path' that avoids ending humanity.

As there is so much uncertainty, they do not even attempt to predict a full decade ahead. This also reflects the difficulties with predicting exponential growth (because of the [recursive self-improvement](https://en.wikipedia.org/wiki/Recursive_self-improvement) they assume future AI agents to be capable of), where even small deviations in the exponent may lead to massively different outcomes (e.g. in terms of timescales). Consequently, the authors only outline what they think will probably happen in the next three to five years.

The scenario is full of **specific predictions and detailed reasoning** explaining potential bottlenecks and other important context, as well as many interesting references and footnotes. 
So even if you are mostly [bored of AI](https://paulrobertlloyd.com/2025/087/a1/bored/) and all the hype right now --- **make this the _one_ piece on AI you read, and you will be well-informed on all big-picture issues**: mostly, how it could be possible that the [AI alignment problem](https://en.wikipedia.org/wiki/AI_alignment) interacts with [geopolitics](https://en.wikipedia.org/wiki/Prisoner's_dilemma#International_politics) and [recursive self-improvement](https://en.wikipedia.org/wiki/Recursive_self-improvement) to all our detriment in the relatively near future.

The prediction also defines **critical milestones, e.g. when models develop specific new capabilities** (such as outperforming humans in programming tasks).
This starts with `Agent-0`, the latest model of the (completely fictitious!^^) US-based market leader in AI, which is called _OpenBrain_. The detailed prediction goes on until `Agent-4`, a model that outperforms humans at AI research and thus will be effective at recursive self-improvement---but is unfortunately misaligned with ['The Spec'](https://openai.com/index/introducing-the-model-spec/) it was given by the humans.

## The groundwork for the AI-2027 predictions

The predictions are based on five fundamental forecasts:

1. The [**timelines forecast**](https://ai-2027.com/research/timelines-forecast) estimates how long it will take until an AI surpasses the best human programmer.

2. Based on this, the [**takeoff forecast**](https://ai-2027.com/research/takeoff-forecast) tries to forecast how long it will _then_ take until an AI surpasses the best human in any cognitive task, i.e. artificial superintelligence.

3. The [**AI goals forecast**](https://ai-2027.com/research/ai-goals-forecast) tries to forecast which goals advanced AIs may have. This is a very detailed report (including a survey of today's LLMs), but it is also quite speculative and the authors actively invite feedback.

4. The [**security forecast**](https://ai-2027.com/research/security-forecast) estimates the time it takes the US and China[^china] to secure both the algorithmic secrets as well as the weights of the latest models regarding certain attack scenarios. Interestingly, securing the weights may become _easier_ with time, as the models become larger and exfiltration will thus require more bandwidth or time (which makes it easier to detect).[^rand]

5. The [**compute forecast**](https://ai-2027.com/research/compute-forecast) estimates the compute power that will be available to the individual actors (states but also companies) to train new AI models.

If you have specific knowledge in any of these areas, it might make sense to start _there_, so that you can see for yourself whether you find the underlying assumptions valid. 
Each of these reports is very detailed and considers several factors to answer each question.

## Alternatively, check out the Dwarkesh podcast on the topic

[Dwarkesh Patel](https://time.com/7012877/dwarkesh-patel/) just interviewed Daniel Kokotajlo (the first author of the scenario) and Scott Alexander about this scenario on [his podcast](https://www.dwarkesh.com/p/scott-daniel) ([here](https://www.youtube.com/watch?v=htOvH12T7mU) is the YouTube video), so you can also watch and listen to their predictions.[^podcast]

Both authors try to make it very clear that this scenario is _not_ a recommendation in any sense: they just think this is the most likely chain of events in the next months.[^p-doom] I hope they regularly update their scenario with a 'diff' while we live through these [interesting times](https://en.wikipedia.org/wiki/May_you_live_in_interesting_times), and I also really hope that Scott Alexander is right in his optimism regarding AI-alignment... ðŸ«£

--

[^scott-alexander]: One author is the blogger Scott Alexander, of [slatestarcodex](https://slatestarcodex.com/) and now---after [doxing by the New York Times](https://www.astralcodexten.com/p/statement-on-new-york-times-article)---[astralcodexten](https://www.astralcodexten.com/) fame.

[^podcast]: This is a three hours recording and I've only listened to the first half so far; it was very insightful. Dwarkesh pulls no punches and asks many critical questions. It is also the first appearance of Scott Alexander in a podcast, in case you wondered what he is like.

[^ai-futures]: This work was done as part of the ['AI futures project'](https://ai-futures.org).

[^rand]: The authors are basing their discussion on the security levels introduced in [this RAND report on AI security](https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2800/RRA2849-1/RAND_RRA2849-1.pdf) on the matter (at the time of writing this link was broken in the forecast document).

[^p-doom]: On the podcast [the authors talk about `p(doom)`](https://youtu.be/htOvH12T7mU?t=5642), i.e. their own subjective estimate of the probability of the 'doomsday' scenario they describe. Kokotajlo said his estimate is `p(doom)=0.7`, while Alexander estimates it as `p(doom)=0.2` (and mentioned that he is more optimistic than any other author of the scenario).

[^gwern]: Another 'classic' in this genre is ['Clippy'](https://gwern.net/fiction/clippy), by [Gwern Branwen](https://gwern.net/me).

[^china]: There are no other states in the scenario that are considered to be relevant actors.