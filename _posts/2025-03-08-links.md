---
title:  "Superintelligence, supermutations, supersad Firefox"
date:   2025-03-08
tags: links ai math firefox security safety game-theory
---

## How to deal with superintelligence


[Eric Schmidt](https://de.wikipedia.org/wiki/Eric_Schmidt) (former CEO of Google) just published, together with two other well-known AI experts, a [new 'whitepaper' on the geopolitical implications of an arms race towards 'superintelligence'](https://www.nationalsecurity.ai) (LessWrong has [a nice overview](https://www.lesswrong.com/posts/Si8dvCpDHxsZTJSLw/aisn-49-superintelligence-strategy#Superintelligence_Strategy)). It's a sobering read, but the core argument is optimistic: they consider it feasible to reach a steady state of _'Mutual Assured AI Malfunction (MAIM)'_:

> [...] a deterrence regime resembling nuclear mutual assured destruction (MAD) where any stateâ€™s
aggressive bid for unilateral AI dominance is met with preventive sabotage by rivals.

They also consider this state reachable with minimal international treaties (only relatively non-controversial ones, like non-proliferation treaties to prevent 'rogue actors' accessing powerful models). 
I'm a bit surprised they do not even discuss [Leopold Aschbrenner's take](https://situational-awareness.ai), which is more pessimistic and contradicts some of their assumptions, e.g. regarding our capabilities to properly secure the weights of LLMs (in their framework, that would be the 'Information Security' line of defense).

Another interesting implication from the paper is that 'AI datacentres' should be treated like nuclear missile silos and should thus be located in remote areas, because "potential kinetic strikes on datacenters" are "relatively easy"[^1]. As they assume we are _already_ in this geopolitical phase, because it is the 'default regime', this is insofar actionable as it means one should avoid living very close to such a datacentre (even if this means higher latency ;-)[^2].

## How to find short supermutations 

As the old adage goes: if something sounds too good to be true, it usually is. It's a bit of hyperbole that [Anime fans _stumbled_ upon a proof](https://www.scientificamerican.com/article/the-surprisingly-difficult-mathematical-proof-that-anime-fans-helped-solve/) (via [HN](https://news.ycombinator.com/item?id=43282133)) for an open combinatorial problem, as the Scientific American tells the story. But it is true that there was an [anonymous post](https://warosu.org/sci/thread/S3751105#p3751197) on a _scientific_ 4chan board where the problem was discussed in terms of TV series episodes, and this ended up being [an important contribution for a proof](https://oeis.org/A180632/a180632.pdf) (note the first author^^).

The problem itself is interesting: given a TV series with `n` episodes, what is the shortest sequence of episodes to watch that contains _every possible permutation_ of the episodes? A string sequence containing all permutations is called a [supermutation](https://en.wikipedia.org/wiki/Superpermutation), and it is trivial to generate one (just concatenate all permutations); the hard part is finding the _minimal_ supermutation size. The proof is based on graph theory and establishes `n! + (n âˆ’ 1)! + (n âˆ’ 2)! + n âˆ’ 3` as the lower bound for supermutation length, with `n` being the number of episodes, and we know for `n â‰¤ 5` that the smallest supermutations has length `1! + 2! + ... + n!`. We don't know any smallest supermutation for `n > 5`.


## How to make Firefox privacy-friendly

As a (part-time) Firefox user [the policy changes detailed in this ZDNET article](https://www.zdnet.com/article/the-firefox-i-loved-is-gone-how-to-protect-your-privacy-on-it-now/) sound like a bad idea. 
Privacy was one of the strong points for Firefox, and with [the user base waning](https://gs.statcounter.com/browser-market-share) (as of last month, Firefox has a desktop browser market share of 6.3%) this does not bode well for the browser.
At least, the article also gives several suggestions of re-configuring Firefox so that it does not phone home.


--

[^1]: [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky), one of the most prominent 'AI Doomers' [already suggested such a policy in March 2023](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

[^2]: BTW I've tried to [google _"superintelligence article eric schmidt"_](https://www.google.com/search?q=superintelligence+article+eric+schmidt&oq=superintelligence+article+eric+schmidt) and different variations of this, but could _not_ make the actual article appear on the first page. Only articles _about_ the article ðŸ™„.

[^3]: See [this HN comment](https://news.ycombinator.com/item?id=43282133).